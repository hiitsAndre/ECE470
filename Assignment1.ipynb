{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiitsAndre/ECE470/blob/master/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZWsnEMjaMR0"
      },
      "source": [
        "**Welcome to the first programming assignment for CS 498 RL!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfPe-_9iawIJ"
      },
      "source": [
        "This assignment will get you familiar with the OpenAI gym environment and estimation via sampling trajectories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEvS3FEgWxc6"
      },
      "source": [
        "import gym"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMgTMDy6OS2e"
      },
      "source": [
        "We will be playing on the Cartpole environment. As the title suggests, the task in this environment is to balance a pole on top of a cart. The official description of the environment from the OpenAI Gym website (https://gym.openai.com/) is:\n",
        "\n",
        "> A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.CartPole-v0 defines \"solving\" as getting average reward of 195.0 over 100 consecutive trials. This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson [Barto83].\n",
        "\n",
        "The environment is loaded as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E9zqEYpOd6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "bab954d7-1d46-4299-8f25-dd203f7af332"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "env.reset() #must reset the environment before interacting with it\n",
        "print (env.action_space)   # show the action space, which has 2 actions\n",
        "print (env.observation_space) #show the state (observation) space, which is a 4-dimensional vector with components of [position,velocity,pole angle, pole velocity at tip]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(2)\n",
            "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p-aroLbSNHc"
      },
      "source": [
        "More information can be found on the following wiki page: https://github.com/openai/gym/wiki/CartPole-v0\n",
        "\n",
        "Here is the policy that you will be using (which just acts randomly with a uniform distribution). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Tj6_VrSSW_0"
      },
      "source": [
        "def policy_unif(s):\n",
        "  a = env.action_space.sample()\n",
        "  return a"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gGjIQIkWKEw"
      },
      "source": [
        " You can interact with the environment with the env.step() function, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFN1pJUaWOqm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "fd07cdaf-0afe-46ee-8c62-59ae5712d05d"
      },
      "source": [
        "observation, reward, done, info = env.step(env.action_space.sample())\n",
        "print(observation)  # the state that you transition to after taking the action\n",
        "print(reward)       # immediate reward \n",
        "print(done)         # a boolean flag of whether the episode has terminated\n",
        "print(info)         # not useful for this assignment"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.30015929 -0.43595157  0.23493729  0.60091032]\n",
            "0.0\n",
            "True\n",
            "{}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg5RqiBMQKKw"
      },
      "source": [
        "**Q1**: Write a method called collect_trajectory(policy) which collects one trajectory for an episode in the Cartpole environment. Your method should take as input the policy and output a list of the form (s0,a0,r0,s1,a1,r1,...,sT,aT,rT), where T is the length of the episode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob_sqnGGQlJC"
      },
      "source": [
        "# generate a trajectory in the environment  \n",
        "# use env.step to roll out a trajectory until the episode terminates\n",
        "# output a list of the form [s0, a0, r0, s1, a1, ..., sT, aT, rT] (s_{T+1} is the terminal state)\n",
        "def collect_trajectory(policy):\n",
        "  s0 = env.reset() #reset the environment\n",
        "  # YOUR CODE HERE\n",
        "  action = policy_unif(s0)\n",
        "  observation, reward, done, info = env.step(action)\n",
        "  # initialize output list\n",
        "  output = [s0, action, reward]\n",
        "  while done != True:\n",
        "    output.append(observation)\n",
        "    action = policy_unif(observation)\n",
        "    output.append(action)\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    output.append(reward)\n",
        "  return output"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwwP8NzVRYL6"
      },
      "source": [
        "**Q2**: Write a method called compute_return, which takes as input one trajectory of the form given by collect_trajectory in Q1 and a discount factor, and calculates the random return of the trajectory, i.e., r1 + γ r2 + γ^2 r3 + ... + γ^{T-1} rT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DBJBZhqRhwa"
      },
      "source": [
        "def compute_return(traj, gamma):\n",
        "  # YOUR CODE HERE\n",
        "  retval = 0\n",
        "  i = 2\n",
        "  while i < len(traj):\n",
        "    retval = retval + pow(gamma, int(i/3)) * traj[i]\n",
        "    i = i + 3\n",
        "    \n",
        "  return retval"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw1Yvbw05uPP"
      },
      "source": [
        "**Q3**: Collect 150 trajectories starting from the initial states given by env.reset() and compute the random return of each trajectory, using the given policy and $\\gamma=0.99$.\r\n",
        "\r\n",
        "1.   Plot a histogram of these returns.\r\n",
        "2.   Estimate the mean of these returns, and give your result in the form of $X \\pm Y$, where $X$ is the estimated mean and $Y$ is twice the standard error of your mean estimate, which corresponds to a 99% confidence interval.\r\n",
        "\r\n",
        "Remark: The mean is also an estimate of the value function of $\\pi$ for the initial state, often referred to as a \"Monte-Carlo\" estimation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYNjz6fS9Vaj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "3f02b5db-855a-42df-fd59-b8df6c7c61a8"
      },
      "source": [
        "# useful libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# use plt.hist to create histogram plot\n",
        "# YOUR CODE HERE\n",
        "ret = []\n",
        "for i in range(150):\n",
        "  traj = collect_trajectory(policy_unif(env.reset()))\n",
        "  rr = compute_return(traj, 0.99)\n",
        "  ret.append(rr)\n",
        "plt.hist(ret)\n",
        "plt.title('histogram plot')\n",
        "\n",
        "print('mean of returns: ', np.mean(ret), u\"\\u00B1\", 2*np.std(ret)/np.sqrt(150-1))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean of returns:  19.759408421188514 ± 1.5243021041026936\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBFD-MBhemrb"
      },
      "source": [
        "**Q4 (Optional)**: A linear policy has parameters $\\beta_1 \\in \\mathbb{R}^4$ and $\\beta_0 \\in \\mathbb{R}$. It computes $\\beta_1^\\top x - \\beta_0$, chooses action $+1$ if $\\beta_1^\\top x - \\beta_0 \\geq 0$, and chooses action $-1$ otherwise.\n",
        "\n",
        "Write a random search learner. Randomly generate 10 linear policies, evaluate them by sampling trajectories, and output the policy with the highest return.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb8YPaYIW8k1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}